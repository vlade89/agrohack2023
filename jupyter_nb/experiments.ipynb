{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c354ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import hdbscan\n",
    "# import umap\n",
    "import sklearn.cluster as cluster\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import clusteval\n",
    "import compress_fasttext\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_score,\n",
    "    completeness_score,\n",
    "    v_measure_score,\n",
    "    silhouette_score\n",
    ")\n",
    "from scipy.linalg import norm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import FastText\n",
    "\n",
    "SEED = 2023\n",
    "VECTOR_SIZE = 16\n",
    "DATA_PATH = '../data/'\n",
    "MODEL_PATH = '../nlp_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d6637f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sys\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sys\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../lib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Preprocessing\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.join(sys.path[0], '..'))\n",
    "sys.path.append(os.path.join(sys.path[0], '../lib'))\n",
    "from ..lib.nlp_utils import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ffdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8eddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Text Preprocessing \"\"\"\n",
    "import logging\n",
    "import re\n",
    "from functools import lru_cache\n",
    "from multiprocessing import Pool\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "wcoll_morph: Optional[pymorphy2.MorphAnalyzer] = None\n",
    "g_chunks: Optional[List[pd.DataFrame]] = None\n",
    "\n",
    "\n",
    "def ensure_morph():\n",
    "    global wcoll_morph\n",
    "    if wcoll_morph is None:\n",
    "        wcoll_morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def release_morph():\n",
    "    global wcoll_morph\n",
    "    wcoll_morph = None\n",
    "\n",
    "\n",
    "def make_chuncks(df, proc_count):\n",
    "    result = []\n",
    "    data_size = df.shape[0]\n",
    "    chunksize = int(np.ceil(data_size / proc_count))\n",
    "    left = right = 0\n",
    "    for i in range(proc_count - 1):\n",
    "        right += chunksize\n",
    "        result.append(df[left:right])\n",
    "        left += chunksize\n",
    "    result.append(df[left:])\n",
    "    return result\n",
    "\n",
    "\n",
    "class Preprocessing:\n",
    "    \"\"\" Clean, tokenize and normalize texts \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(get_stop_words(\"ru\"))\n",
    "        self.pattern = re.compile(\"[А-Яа-яA-z0-9]+\")\n",
    "\n",
    "    def process_texts(self, df, text_col, proc_count=1):\n",
    "        ensure_morph()\n",
    "        df[\"tokens\"] = df[text_col].str.lower().str.findall(self.pattern)\n",
    "        try:\n",
    "            if proc_count == 1:\n",
    "                df[\"tokens\"] = df[\"tokens\"].apply(\n",
    "                    lambda txt: self.tokenize(txt)\n",
    "                )\n",
    "            else:\n",
    "                global g_chunks\n",
    "                logging.info(\"Reading chunks ...\")\n",
    "                g_chunks = list(make_chuncks(df[\"tokens\"], proc_count=proc_count))\n",
    "                logging.info(\n",
    "                    \"Chunk count %s %s\",\n",
    "                    len(g_chunks),\n",
    "                    sum(ch.shape[0] for ch in g_chunks),\n",
    "                )\n",
    "                logging.info(\"Processing chunks ...\")\n",
    "                with Pool(proc_count) as p:\n",
    "                    result = p.map(self.process_series, range(len(g_chunks)))\n",
    "                df[\"tokens\"] = pd.concat(result)\n",
    "                g_chunks = None\n",
    "        finally:\n",
    "            release_morph()\n",
    "        return df[\"tokens\"]\n",
    "\n",
    "    @lru_cache(maxsize=50000)\n",
    "    def normalize_word(self, token):\n",
    "        \"\"\"\n",
    "        Pymorphy2 normalizer.\n",
    "\n",
    "        Args:\n",
    "            token: str\n",
    "                token to normalize\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        global wcoll_morph\n",
    "\n",
    "        return wcoll_morph.parse(token)[0].normal_form\n",
    "\n",
    "    def tokenize(self, arr):\n",
    "        \"\"\"\n",
    "        Tokenizes, normalizes input text, removes stop-words.\n",
    "\n",
    "        Args:\n",
    "            arr: List[str]\n",
    "                list of tokens\n",
    "        Returns:\n",
    "            list of integers\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.normalize_word(t.strip())\n",
    "            for t in arr\n",
    "            if t not in self.stopwords and len(t) > 2\n",
    "        ]\n",
    "\n",
    "    def process_series(self, chunk_num: int) -> pd.Series:\n",
    "        global g_chunks\n",
    "        return g_chunks[chunk_num].apply(lambda txt: self.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resumes.pickle\", \"rb\") as f:\n",
    "    new_resumes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1991f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(MODEL_PATH + 'small_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35c114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
